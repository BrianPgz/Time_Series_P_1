---
title: "Beer"
author: "Brian Perez G"
date: "14/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r echo=FALSE}
library(forecast)
library(timeSeries)
library(nortest)
library(tseries)
library(TSA)
library(lmtest)
library(ggplot2)

```



```{r echo=TRUE}
Data_beer= read.csv(file = "monthly_beer.csv")
```

```{r echo=TRUE}
head(Data_beer)
```

Tenemos $Data_beer$ en data frame tenemos que tener el formato de serie de tiempo. 

Vamos a obtener la primera y la última fecha.

```{r echo=TRUE}
tamanio_base = length(Data_beer$Month)
tamanio_base
```

Tenemos que la base tiene 476 datos. 

```{r echo=TRUE}
Data_beer$Month[1]
Data_beer$Month[476]
```

La base tiene fecha de inicio en enero de 1956 y fin en agosto de 1995. Vamos a crear el objeto serie de tiempo.

```{r echo=TRUE}
S_beer = ts(Data_beer$Monthly.beer.production, start = c(1956, 1), end = c(1995, 8), frequency = 12)
head(S_beer,24)
```

Vamos a visualizar la serie de tiempo. 

```{r echo=TRUE}
ts.plot(S_beer,main = "Serie de tiempo de producción mensual de cerveza")
```

Obtengamos algunas medidas importantes. 

```{r echo=TRUE}
mean(S_beer)
var(S_beer)
sd(S_beer)
```

Tenemos que mensualmente en promedio de produce $136.3954$ millones de litros de cerveza con una desviación estándar de $33.73872$ millones de litros de cerveza al mes. 


Veamos los correlogramas de la serie de tiempo. 

```{r echo=TRUE}
tsdisplay(S_beer, main = "Serie de tiempo de producción mensual de cerveza")
```

Observamos varianza creciente, al principio se ve puede observar tendencia pero cerca de 1980 se rompe la tendencia, o bien, la tendencia cambia. 

En el $ACF$ Observamos alta correlación y que además no decae la correlación de manera exponencial. Para el $PACF$ observamos que la correlación parcial sigue siendo alta y por ello aproximadamente 15 barras se salen de las barras de confianza.  


Podemos observar ciclos con un periodo igual a 12. 

```{r echo=TRUE}
desc_datos = decompose(S_beer)
plot(desc_datos)   

```

Observamos que la tendencia cambia y tenemos ciclos, además del componente aleatorio. Vamos a hacer pruebas, y si es el caso haremos transformaciones para arreglar la serie.   

Haciendo el $bp \ test$ vamos a contrastar la varianza. 

$Ho:$ La varianza es constante (Homocedasticidad)   $vs.$  $H1:$ La varianza no es constate (Heterocedasticidad)

```{r echo=TRUE}
t1=seq(1956,1995.59,by=1/12)
bptest( S_beer ~ t1)  
```

Con un $p-value$ de casi cero entonces rechazamos $Ho$, entonces tenemos varianza no constante. 

Vamos a hacer la prueba de Dickey-Fuller

$Ho:$ La serie no es estacionaria $vs.$ $H1:$ La serie es estacionaria


```{r echo=TRUE}
adf.test(S_beer) 
```

Con un $p-value = 0.01$, y un nivel de confianza del $.05$, rechazamos hipótesis nula, entonces es estacionaria. 


Para contrastar nuestros resultados vamos a usar la prueba de phillips. 

$Ho:$ La serie es estacionaria $vs.$ $H1:$ La serie no es estacionaria

```{r echo=TRUE}
kpss.test(S_beer)
```
Con un $p-value=0.01$ rechazamos $Ho$ entonces la serie no es estacionaria. Como esto contradice a la prueba anterior debemos de trabajar con una trandformación de la serie.

Para el periodo vamos a usar la función $cycle()$

```{r echo=TRUE}
plot(cycle(S_beer),main="Ciclos") 
```

Como la serie va de 1956 a 1995, en la gráfica observamos 10 ciclos cada 10 años, entonces el ciclo se repite anualmente, con un periodo $d=12$ en meses. 


Veamos primero qué pasa con el logaritmo de la serie. 


```{r echo=TRUE}
log_S_beer = log(S_beer)
ts.plot(log_S_beer, main= "logaritmo de la serie")

```

Parece ser que la varianza se estabiliza pero seguimos teniendo problemas con la tendencia. 

```{r echo=TRUE}
bptest(log_S_beer~ t1)

```

Con un $p-value = 0.3914$ la varianza es constante. 


```{r echo=TRUE}
adf.test(log_S_beer) 
```

Con un $p-value=0.03871$ menor al $.05$, rechazamos $Ho$ entonces la serie es estacionaria. Para contrastar, tenemos que hacer la prueba de phillips. 

$Ho:$ La serie es estacionaria $vs.$ $H1:$ La serie no es estacionaria

```{r echo=TRUE}
kpss.test(log_S_beer)
```
Para la transformación logaritmo se siguen contradiciendo las pruebas. Aunque tenemos varianza constante. 



```{r echo=TRUE}
desc_datos = decompose(log_S_beer)
plot(desc_datos)  
```

Con el logaritmo obtenemos varianza constante, seguimos viendo ciclos anuales aunque seguimos teniendo problemas con la tendencia. 


Queremos usar métodos de descomposición adecuado. Primero veamos modelos de regresión. 


```{r echo=TRUE}
time=t1
time2=t1*t1
time3=t1*t1*t1
regresion_simple= lm(log_S_beer ~ time)
plot(log_S_beer,main="Serie con varianza constante (Logaritmo)",lwd = 3, xlab = "Tiempo", col = "black")
lines(time,regresion_simple$fitted.values,col="blue",lwd = 3)
```

Parece ser que un modelo lineal no es la mejor opción, usemos un polinomio. 

```{r echo=TRUE}
time=t1
time2=t1*t1
time3=t1*t1*t1
regresion_square= lm(log_S_beer ~ time + time2)
plot(log_S_beer,main="Serie con varianza constante (Logaritmo)",lwd = 3, xlab = "Tiempo", col = "black")
lines(time,regresion_square$fitted.values,col="blue",lwd = 3)
```

Parece que un polinomio cuadratico ajusta bien a la serie de tiempo. 

Vamos a predecir tres años futuros, primero vamos a llenar los datos que se van a predecir con NA.


```{r echo=TRUE}
time_plus=seq(1956,1998.59,by=1/12)
time_plus2=time_plus*time_plus
time_plus3=time_plus*time_plus*time_plus
df_pred <- data.frame(X = time_plus,Y=time_plus2 , Z = c(log_S_beer, rep(NA, 36) ))
```

```{r echo=TRUE}

predictions <- lm(formula = Z ~ X + Y, data = df_pred)
predictor=predict(object = predictions, newdata = df_pred) 
plot_serie=plot(log_S_beer,main="Serie con varianza constante (Logaritmo y Prediccion de 3 años)",lwd = 3, xlab = "Tiempo", col = "black",xlim=c(1956,1999))
lines(time_plus,predictor,col="blue",lwd = 3)

```

Haciendo la predicción con el polinomio de grado dos observamos el cambio de la tendencia cerca del año 1980.


Veamos qué pasa cuando aplicamos una diferencia a los datos bajo la transformación logaritmo. 

```{r echo=TRUE}
diff_S_berr = diff(log_S_beer)
plot(diff_S_berr)   

```

Con este método quitamos la tendencia, se observan ciclos pero tenemos varianza constante, procedemos a hacer las pruebas. 

Con las diferencias perdemos datos. 

```{r echo=TRUE}
new_t= t1[2:(length(t1))]
bptest(diff_S_berr ~ new_t)
```

Rechazamos hipótesis nula entonces no tiene varianza constante.





```{r echo=TRUE}
adf.test(diff_S_berr)
```
Obtenemos que la serie es estacionaria.

```{r echo=TRUE}
kpss.test(diff_S_berr)
```

Tenemos que aceptar hipótesis nula, entonces es una serie estacionaria.

Ahora ya se cumple que la diferencia de la serie con logaritmo es estacionaria, por ambos test.  

Tenemos que ajustar el modelo ARIMA O SARIMA, primero veamos qué pasa para los datos originales.



```{r echo=TRUE}
model1=auto.arima(S_beer)
summary(model1)
```

Para los datos originales obtenemos un SARIMA$(0,1,3) \times (1,1,2) [12]$. Procedemos a comprobar supuestos. 

Queremos ver que los coeficientes sean significativos. 


```{r echo=TRUE}
confint(model1)
```

Como algunos intervalos de los coeficientes intersectan a cero, son no significativos (ma2,sar1,sma2).

Veamos si los residuales siguen una distribución normal.

```{r echo=TRUE}
qqnorm(model1$residuals)
qqline(model1$residuals)
```


```{r echo=TRUE}
jarque.bera.test(model1$residuals)
```

Con un $p-value$ de casi cero, obtenemos no normalidad de residuales.


```{r echo=TRUE}
shapiro.test(model1$residuals)
```
Con un $p-value$ de casi cero, obtenemos no normalidad de residuales con ambas pruebas. 


```{r echo=TRUE}
time_var= 1:length(model1$residuals)
residuals_M1=as.numeric(model1$residuals)
bptest(residuals_M1~time_var)
```

Los residuales no pasaron el test de varianza constante. 

Para ver que tengamos media cero, hacemos: 


```{r echo=TRUE}
t.test(model1$residuals,mu = 0)
```
Con un $p-value=0.7171$ no rechazamos hipótesis nula, entonces tiene media cero. 

Veamos la correlación de los residuales.

```{r echo=TRUE}
tsdisplay(model1$residuals)
```

Vemos que tenemos problemas con la correlación ya que muchas barras se salen de las bandas de confianza. Para ver si realmente tenemos problemas con la correlación vamos a usar un tsdiag. 


```{r echo=TRUE}
tsdiag(model1,gof.lag = 50)
```

Observamos que en varios lag se salen de las bandas de coinfianza, vemos que están muy correlacionados los residuales. Tenemos valores para los que el $p-value$ general está por debajo de las bandas.  

Con el $model1$ el problema que tenemos es la varianza no constante y la alta correlación de los residuales, además falla en los supuestos de normalidad y el $p-value$ para Ljung-Box. 


Ajustemos otro modelo, en base a las diferencias y el logaritmo. 

```{r echo=TRUE}
Model2=auto.arima(diff_S_berr)
summary(Model2)
```

Por medio del comando auto.arima obtenemos un SARIMA$(1,0,0)\times(2,0,0) [12]$.



```{r echo=TRUE}
confint(Model2)
```
Tenemos coeficientes significativos, no intersectan a cero. 


```{r echo=TRUE}
time_var_2= 1:length(Model2$residuals)
residuals_M2=as.numeric(Model2$residuals)
bptest(residuals_M2~time_var_2)
```
Tenemos varianza no constante. 


Veamos si los residuales son normales. 


```{r echo=TRUE}
adf.test(Model2$residuals)
kpss.test(Model2$residuals)
jarque.bera.test(Model2$residuals)
shapiro.test(Model2$residuals)
```

En este caso aceptamos $Ho$, entonces los residuales siguen una distribución normal. 

```{r echo=TRUE}
qqnorm(Model2$residuals)
qqline(Model2$residuals)
```

Ahora que tenemos normalidad veamos, la media. 

```{r echo=TRUE}
t.test(Model2$residuals,mu = 0)
```

Tenemos media cero. 

```{r echo=TRUE}
tsdiag(Model2, gof.lag = 50)
```
Observamos que para el $Model2$ seguimos teniendo los problemas del modelos anterior. 


Vamos a proponer un mejor modelo, cambiando los coeficientes. Necesitamos una serie con varianza constante. 

Proponemos la transformación diferencia del logaritmo de la serie. 






